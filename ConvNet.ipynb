{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Gender Recognition Through Face Using Deep Learning\n",
    "<img src=\"images/[3].png\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, matplotlib.pyplot as plt, numpy as np, os\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Lambda\n",
    "from keras.utils import plot_model\n",
    "from helper.prepare_data import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we pick the dataset that we would like to use from:\n",
    "<br>1 - Age, gender, ethnicity CSV\n",
    "<br>2 - UTKFace\n",
    "<br>3 - Fairface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data from each dataset so that we end up with normalized pixels and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_age, y_gender, y_ethnicity, img_size = prepare_data(choice = choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some exemplary images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the whole grid of images\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# display first 25 images in a 5x5 grid\n",
    "for i in range(0, 25):\n",
    "    plt.subplot(5, 5, (i % 25) + 1)\n",
    "    plt.grid(False)\n",
    "    #disable x and y axis description\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(x[i].reshape(img_size, img_size), cmap='gray')\n",
    "    # A = age, G = gender, E = ethnicity\n",
    "    plt.xlabel(\"A: \"+ str(y_age[i]) + \" G: \" + str(y_gender[i]) + (\" E: \" + str(y_ethnicity[i]) if y_ethnicity.size > 0 else \"\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((img_size, img_size, 1))\n",
    "\n",
    "layer = Conv2D(64, kernel_size = (3, 3), activation = 'relu', name = 'conv2d_1') (inputs)\n",
    "layer = MaxPooling2D(pool_size = (2, 2)) (layer)\n",
    "\n",
    "# LRN layer\n",
    "# layer = Lambda(tf.nn.local_response_normalization) (layer)\n",
    "\n",
    "layer = Conv2D(128, kernel_size = (3, 3), activation = 'relu', name = 'conv2d_2') (layer)\n",
    "layer = MaxPooling2D(pool_size = (2, 2)) (layer)\n",
    "\n",
    "layer = Conv2D(256, kernel_size = (3, 3), activation = 'relu', name = 'conv2d_3') (layer)\n",
    "layer = Conv2D(256, kernel_size = (3, 3), activation = 'relu', name = 'conv2d_4') (layer)\n",
    "layer = MaxPooling2D(pool_size = (2, 2)) (layer)\n",
    "\n",
    "layer = Conv2D(512, kernel_size = (3, 3), activation = 'relu', name = 'conv2d_5') (layer)\n",
    "layer = Conv2D(512, kernel_size = (3, 3), activation = 'relu', name = 'conv2d_6') (layer)\n",
    "layer = MaxPooling2D(pool_size = (2, 2)) (layer)\n",
    "\n",
    "layer = Conv2D(512, kernel_size = (3, 3), activation = 'relu', name = 'conv2d_7') (layer)\n",
    "layer = Conv2D(512, kernel_size = (3, 3), activation = 'relu', name = 'conv2d_8') (layer)\n",
    "layer = MaxPooling2D(pool_size = (2, 2)) (layer)\n",
    "\n",
    "layer = Flatten() (layer)\n",
    "\n",
    "Dense_1 = Dense(4096, activation='relu') (layer)\n",
    "Dense_2 = Dense(4096, activation='relu') (Dense_1)\n",
    "\n",
    "Dense_output_1 = Dense(1, activation='sigmoid', name = 'gender_out') (Dense_2)\n",
    "Dense_output_2  = Dense(1, activation='relu', name = 'age_out') (Dense_2)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [Dense_output_1, Dense_output_2])\n",
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001), loss = ['binary_crossentropy', 'mse'], metrics = ['accuracy', 'mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file = os.getcwd() + '//images//models//convnet_model.png', show_shapes = True, show_dtype = False, show_layer_names = False, rankdir = 'TB', expand_nested = False, dpi = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x, [y_gender, y_age] , epochs = 20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_acc = history.history['gender_out_mae']\n",
    "age_val_acc = history.history['val_gender_out_mae']\n",
    "epochs = range(len(age_acc))\n",
    "\n",
    "plt.plot(epochs, age_acc, 'b', label='Training Mean Absolute Error')\n",
    "plt.plot(epochs, age_val_acc, 'r', label='Validation Mean Absolute Error')\n",
    "plt.title('Gender Mean Absolute Error Graph')\n",
    "plt.xticks(np.arange(min(epochs), max(epochs)+2, 2.0))\n",
    "plt.legend()\n",
    "plt.savefig(os.getcwd() + '//images//plots//convnet_gender_mae_ds_' + str(choice), bbox_inches='tight')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_acc = history.history['gender_out_accuracy']\n",
    "age_val_acc = history.history['val_gender_out_accuracy']\n",
    "epochs = range(len(age_acc))\n",
    "\n",
    "plt.plot(epochs, age_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, age_val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Gender Accuracy Graph')\n",
    "plt.xticks(np.arange(min(epochs), max(epochs)+2, 2.0))\n",
    "plt.legend()\n",
    "plt.savefig(os.getcwd() + '//images//plots//convnet_gender_accuracy_ds_' + str(choice), bbox_inches='tight')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_acc = history.history['age_out_mae']\n",
    "age_val_acc = history.history['val_age_out_mae']\n",
    "epochs = range(len(age_acc))\n",
    "\n",
    "plt.plot(epochs, age_acc, 'b', label='Training Mean Absolute Error')\n",
    "plt.plot(epochs, age_val_acc, 'r', label='Validation Mean Absolute Error')\n",
    "plt.title('Age Mean Absolute Error Graph')\n",
    "plt.xticks(np.arange(min(epochs), max(epochs)+2, 2.0))\n",
    "plt.legend()\n",
    "plt.savefig(os.getcwd() + '//images//plots//convnet_age_mae_ds_' + str(choice), bbox_inches='tight')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_acc = history.history['age_out_accuracy']\n",
    "age_val_acc = history.history['val_age_out_accuracy']\n",
    "epochs = range(len(age_acc))\n",
    "\n",
    "plt.plot(epochs, age_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, age_val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Age Accuracy Graph')\n",
    "plt.xticks(np.arange(min(epochs), max(epochs)+2, 2.0))\n",
    "plt.legend()\n",
    "plt.savefig(os.getcwd() + '//images//plots//convnet_age_accuracy_ds_' + str(choice), bbox_inches='tight')\n",
    "plt.figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
